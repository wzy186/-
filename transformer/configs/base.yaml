
# Transformer Model Configuration
model:
  d_model: 128
  num_heads: 4
  num_layers: 2
  d_ff: 512
  dropout: 0.1
  max_seq_len: 1000
  activation: "gelu"

# Training Configuration
training:
  batch_size: 32
  seq_len: 128
  learning_rate: 0.0003
  weight_decay: 0.01
  epochs: 10
  grad_clip: 1.0
  warmup_steps: 1000

# Data Configuration
data:
  dataset: "tiny_shakespeare"
  train_ratio: 0.9
  vocab_size: 10000

# Experiment Configuration
experiment:
  seed: 42
  device: "cuda"  # auto-detect
  save_dir: "../results"
  log_interval: 50